{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Detailed analysis of Concepts and Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "features = boston.data[:, 0:2]\n",
    "target = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "model = lr.fit(features, target)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Variance and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem\n",
    "`You want to reduce the variance of your linear regression model`\n",
    "\n",
    "Solution\n",
    "`Use a learning algorithm that includes a shrinkage penalty (also called regularization) like ridge regression and lasso regression:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "features = boston.data[:, 0:2]\n",
    "target = boston.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.320e-03, 1.800e+01],\n",
       "       [2.731e-02, 0.000e+00],\n",
       "       [2.729e-02, 0.000e+00],\n",
       "       [3.237e-02, 0.000e+00],\n",
       "       [6.905e-02, 0.000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41978194,  0.28482986],\n",
       "       [-0.41733926, -0.48772236],\n",
       "       [-0.41734159, -0.48772236],\n",
       "       ...,\n",
       "       [-0.41344658, -0.48772236],\n",
       "       [-0.40776407, -0.48772236],\n",
       "       [-0.41500016, -0.48772236]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(features)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ridge = Ridge(alpha = 0.5)\n",
    "model_ridge = reg_ridge.fit(scaled, target)\n",
    "model_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In standard linear regression the model trains to minimize the sum of squared error between the true( ùë¶ùëñ ) and prediction ( ùë¶ÃÇ ùëñ ) target values, or residual sum of squares (RSS):\n",
    "\n",
    "- Regularized regression learners are similar, except they attempt to minimize RSS and some penalty for the total size of the coefficient values, called a shrinkage penalty because it attempts to \"shrink\" the model. \n",
    "\n",
    "- There are two common types of regularized learners for linear regression: ridge regression and the lasso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So which one should we use?\n",
    "\n",
    "A a very general rule of thumb, `ridge regression often produces slightly better predictions than lasso, but lasso  produces more interpretable models.` If we want a `balance between, ridge and lasso's penalty functions we can use elastic net, which is simply a regression model with both penalties included.` Regardless of which one we use, both ridge and lasso regresions can penalize large or complex models by including coefficient values in the loss funciton we are trying to minimize\n",
    "\n",
    "`The hyper parameter  ùõº  lets us control how much we penalize the coefficients, with higher values of  ùõº  creating simpler models. The ideal value of  ùõº  should be tuned like any other hyperparameter. In scikit-learn,  ùõº  is set using the alpha parameter.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best Alpha parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `scikit-learn includes a RidgeCV method that allows us to select the ideal value for ùõº:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "reg_cv = RidgeCV([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "model_cv = reg_cv.fit(scaled, target)\n",
    "model_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Because in linear regression the value of the coefficients is partially determined by the scale of the feature, and in regularized models all coefficients are summed together, we must make sure to standardize the feature prior to training`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Features with Lasso Regression¬∂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem\n",
    "`You want to simplify your linear regression model by reducing the number of features.`\n",
    "\n",
    "Solution\n",
    "`Use a lasso regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "features = boston.data\n",
    "target = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41978194,  0.28482986, -1.2879095 , ..., -1.45900038,\n",
       "         0.44105193, -1.0755623 ],\n",
       "       [-0.41733926, -0.48772236, -0.59338101, ..., -0.30309415,\n",
       "         0.44105193, -0.49243937],\n",
       "       [-0.41734159, -0.48772236, -0.59338101, ..., -0.30309415,\n",
       "         0.39642699, -1.2087274 ],\n",
       "       ...,\n",
       "       [-0.41344658, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.44105193, -0.98304761],\n",
       "       [-0.40776407, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.4032249 , -0.86530163],\n",
       "       [-0.41500016, -0.48772236,  0.11573841, ...,  1.17646583,\n",
       "         0.44105193, -0.66905833]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale = StandardScaler()\n",
    "scaled = scale.fit_transform(features)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_lasso = Lasso(alpha = 0.5)\n",
    "model_lasso = reg_lasso.fit(scaled, target)\n",
    "model_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`One interesting characteristic of lasso regression's penalty is that it can shrink the coefficients of a model to zero, effectively reducing the number of features in the model. For example, in our solution we set alpha to 0.5 and we can see that many of the coefficients are 0, meaning their corresponding features are not used in the model:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11526463,  0.        , -0.        ,  0.39707879, -0.        ,\n",
       "        2.97425861, -0.        , -0.17056942, -0.        , -0.        ,\n",
       "       -1.59844856,  0.54313871, -3.66614361])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`However if we increase  ùõº  to a much higher value, we see that lierally none of the features are being used:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.,  0., -0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,  0., -0.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_lasso_10 = Lasso(alpha = 10)\n",
    "model_lasso_10 = reg_lasso_10.fit(scaled, target)\n",
    "model_lasso_10.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`The practical benefit of this effect is that it means that we could include 100 features in our feature matrix and then, through adjusting lasso's  ùõº  hyperparameter, produce a model that uses only 10 (for instance) of the most important features. This lets us reduce variance while improving interpretability of our model (since fewer features is easier to explain)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Despite being called a regression, logistic regression is actually a widely used supervised classification technique. Allows us to predict the probability that an observation is of a certain class`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.81065904e-01,  8.41837140e-01, -1.01297765e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-8.94308978e-01, -2.07835104e-01, -1.01297765e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-1.20755205e+00,  2.12033793e-01, -1.08231219e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-1.36417359e+00,  2.09934449e-03, -9.43643106e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-7.37687441e-01,  1.05177159e+00, -1.01297765e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-1.11201292e-01,  1.68157493e+00, -8.04974023e-01,\n",
       "        -6.86441647e-01],\n",
       "       [-1.36417359e+00,  6.31902691e-01, -1.01297765e+00,\n",
       "        -8.64276271e-01],\n",
       "       [-7.37687441e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-1.67741667e+00, -4.17769553e-01, -1.01297765e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-8.94308978e-01,  2.09934449e-03, -9.43643106e-01,\n",
       "        -1.21994552e+00],\n",
       "       [-1.11201292e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-1.05093052e+00,  6.31902691e-01, -8.74308565e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-1.05093052e+00, -2.07835104e-01, -1.01297765e+00,\n",
       "        -1.21994552e+00],\n",
       "       [-1.83403820e+00, -2.07835104e-01, -1.22098127e+00,\n",
       "        -1.21994552e+00],\n",
       "       [ 5.15284858e-01,  1.89150938e+00, -1.15164673e+00,\n",
       "        -1.04211089e+00],\n",
       "       [ 3.58663321e-01,  2.73124718e+00, -9.43643106e-01,\n",
       "        -6.86441647e-01],\n",
       "       [-1.11201292e-01,  1.68157493e+00, -1.08231219e+00,\n",
       "        -6.86441647e-01],\n",
       "       [-5.81065904e-01,  8.41837140e-01, -1.01297765e+00,\n",
       "        -8.64276271e-01],\n",
       "       [ 3.58663321e-01,  1.47164049e+00, -8.04974023e-01,\n",
       "        -8.64276271e-01],\n",
       "       [-5.81065904e-01,  1.47164049e+00, -9.43643106e-01,\n",
       "        -8.64276271e-01],\n",
       "       [-1.11201292e-01,  6.31902691e-01, -8.04974023e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-5.81065904e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "        -6.86441647e-01],\n",
       "       [-1.36417359e+00,  1.05177159e+00, -1.29031581e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-5.81065904e-01,  4.21968242e-01, -8.04974023e-01,\n",
       "        -5.08607024e-01],\n",
       "       [-1.05093052e+00,  6.31902691e-01, -6.66304941e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-7.37687441e-01, -2.07835104e-01, -8.74308565e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-7.37687441e-01,  6.31902691e-01, -8.74308565e-01,\n",
       "        -6.86441647e-01],\n",
       "       [-4.24444366e-01,  8.41837140e-01, -9.43643106e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-4.24444366e-01,  6.31902691e-01, -1.01297765e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-1.20755205e+00,  2.12033793e-01, -8.74308565e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-1.05093052e+00,  2.09934449e-03, -8.74308565e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-1.11201292e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "        -6.86441647e-01],\n",
       "       [-4.24444366e-01,  2.10144383e+00, -9.43643106e-01,\n",
       "        -1.21994552e+00],\n",
       "       [ 4.54202458e-02,  2.31137828e+00, -1.01297765e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-8.94308978e-01,  2.09934449e-03, -9.43643106e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-7.37687441e-01,  2.12033793e-01, -1.15164673e+00,\n",
       "        -1.04211089e+00],\n",
       "       [ 4.54202458e-02,  8.41837140e-01, -1.08231219e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-8.94308978e-01,  1.05177159e+00, -1.01297765e+00,\n",
       "        -1.21994552e+00],\n",
       "       [-1.67741667e+00, -2.07835104e-01, -1.08231219e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-5.81065904e-01,  6.31902691e-01, -9.43643106e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-7.37687441e-01,  8.41837140e-01, -1.08231219e+00,\n",
       "        -8.64276271e-01],\n",
       "       [-1.52079513e+00, -1.67737625e+00, -1.08231219e+00,\n",
       "        -8.64276271e-01],\n",
       "       [-1.67741667e+00,  2.12033793e-01, -1.08231219e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-7.37687441e-01,  8.41837140e-01, -8.74308565e-01,\n",
       "        -3.30772400e-01],\n",
       "       [-5.81065904e-01,  1.47164049e+00, -6.66304941e-01,\n",
       "        -6.86441647e-01],\n",
       "       [-1.05093052e+00, -2.07835104e-01, -1.01297765e+00,\n",
       "        -8.64276271e-01],\n",
       "       [-5.81065904e-01,  1.47164049e+00, -8.74308565e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-1.36417359e+00,  2.12033793e-01, -1.01297765e+00,\n",
       "        -1.04211089e+00],\n",
       "       [-2.67822829e-01,  1.26170604e+00, -9.43643106e-01,\n",
       "        -1.04211089e+00],\n",
       "       [-7.37687441e-01,  4.21968242e-01, -1.01297765e+00,\n",
       "        -1.04211089e+00],\n",
       "       [ 2.39474331e+00,  2.12033793e-01,  1.27506221e+00,\n",
       "         1.09190459e+00],\n",
       "       [ 1.45501408e+00,  2.12033793e-01,  1.13639313e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 2.23812177e+00,  2.09934449e-03,  1.41373130e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 4.54202458e-02, -1.67737625e+00,  7.89720424e-01,\n",
       "         9.14069966e-01],\n",
       "       [ 1.61163562e+00, -6.27704002e-01,  1.20572767e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 3.58663321e-01, -6.27704002e-01,  1.13639313e+00,\n",
       "         9.14069966e-01],\n",
       "       [ 1.29839254e+00,  4.21968242e-01,  1.27506221e+00,\n",
       "         1.44757384e+00],\n",
       "       [-8.94308978e-01, -1.46744180e+00,  3.04378636e-01,\n",
       "         3.80566095e-01],\n",
       "       [ 1.76825716e+00, -4.17769553e-01,  1.20572767e+00,\n",
       "         9.14069966e-01],\n",
       "       [-4.24444366e-01, -8.37638451e-01,  7.20385883e-01,\n",
       "         1.09190459e+00],\n",
       "       [-7.37687441e-01, -2.30717959e+00,  4.43047718e-01,\n",
       "         3.80566095e-01],\n",
       "       [ 6.71906395e-01, -2.07835104e-01,  9.28389507e-01,\n",
       "         1.26973921e+00],\n",
       "       [ 8.28527933e-01, -1.88731069e+00,  7.89720424e-01,\n",
       "         3.80566095e-01],\n",
       "       [ 9.85149470e-01, -4.17769553e-01,  1.27506221e+00,\n",
       "         1.09190459e+00],\n",
       "       [ 2.02041783e-01, -4.17769553e-01,  5.12382260e-01,\n",
       "         9.14069966e-01],\n",
       "       [ 1.92487869e+00,  2.09934449e-03,  1.06705859e+00,\n",
       "         1.09190459e+00],\n",
       "       [ 2.02041783e-01, -2.07835104e-01,  1.13639313e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 5.15284858e-01, -8.37638451e-01,  8.59054966e-01,\n",
       "         3.80566095e-01],\n",
       "       [ 1.14177101e+00, -1.88731069e+00,  1.13639313e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 2.02041783e-01, -1.25750735e+00,  7.20385883e-01,\n",
       "         5.58400718e-01],\n",
       "       [ 6.71906395e-01,  2.12033793e-01,  1.34439675e+00,\n",
       "         1.80324308e+00],\n",
       "       [ 9.85149470e-01, -6.27704002e-01,  7.89720424e-01,\n",
       "         9.14069966e-01],\n",
       "       [ 1.29839254e+00, -1.25750735e+00,  1.41373130e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 9.85149470e-01, -6.27704002e-01,  1.27506221e+00,\n",
       "         7.36235342e-01],\n",
       "       [ 1.45501408e+00, -4.17769553e-01,  9.97724048e-01,\n",
       "         9.14069966e-01],\n",
       "       [ 1.76825716e+00, -2.07835104e-01,  1.06705859e+00,\n",
       "         1.09190459e+00],\n",
       "       [ 2.08150023e+00, -6.27704002e-01,  1.34439675e+00,\n",
       "         1.09190459e+00],\n",
       "       [ 1.92487869e+00, -2.07835104e-01,  1.48306584e+00,\n",
       "         1.62540846e+00],\n",
       "       [ 8.28527933e-01, -4.17769553e-01,  1.13639313e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 3.58663321e-01, -1.04757290e+00,  4.43047718e-01,\n",
       "         3.80566095e-01],\n",
       "       [ 4.54202458e-02, -1.46744180e+00,  6.51051342e-01,\n",
       "         5.58400718e-01],\n",
       "       [ 4.54202458e-02, -1.46744180e+00,  5.81716801e-01,\n",
       "         3.80566095e-01],\n",
       "       [ 5.15284858e-01, -8.37638451e-01,  7.20385883e-01,\n",
       "         7.36235342e-01],\n",
       "       [ 8.28527933e-01, -8.37638451e-01,  1.55240038e+00,\n",
       "         1.44757384e+00],\n",
       "       [-1.11201292e-01, -2.07835104e-01,  1.13639313e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 8.28527933e-01,  6.31902691e-01,  1.13639313e+00,\n",
       "         1.44757384e+00],\n",
       "       [ 1.92487869e+00,  2.09934449e-03,  1.27506221e+00,\n",
       "         1.26973921e+00],\n",
       "       [ 1.29839254e+00, -1.67737625e+00,  1.06705859e+00,\n",
       "         9.14069966e-01],\n",
       "       [ 2.02041783e-01, -2.07835104e-01,  8.59054966e-01,\n",
       "         9.14069966e-01],\n",
       "       [ 4.54202458e-02, -1.25750735e+00,  7.89720424e-01,\n",
       "         9.14069966e-01],\n",
       "       [ 4.54202458e-02, -1.04757290e+00,  1.06705859e+00,\n",
       "         7.36235342e-01],\n",
       "       [ 9.85149470e-01, -2.07835104e-01,  1.20572767e+00,\n",
       "         1.09190459e+00],\n",
       "       [ 5.15284858e-01, -1.04757290e+00,  7.89720424e-01,\n",
       "         7.36235342e-01],\n",
       "       [-7.37687441e-01, -1.67737625e+00,  3.04378636e-01,\n",
       "         3.80566095e-01],\n",
       "       [ 2.02041783e-01, -8.37638451e-01,  9.28389507e-01,\n",
       "         9.14069966e-01],\n",
       "       [ 3.58663321e-01, -2.07835104e-01,  9.28389507e-01,\n",
       "         7.36235342e-01],\n",
       "       [ 3.58663321e-01, -4.17769553e-01,  9.28389507e-01,\n",
       "         9.14069966e-01],\n",
       "       [ 1.14177101e+00, -4.17769553e-01,  9.97724048e-01,\n",
       "         9.14069966e-01],\n",
       "       [-5.81065904e-01, -1.25750735e+00,  9.63750123e-02,\n",
       "         5.58400718e-01],\n",
       "       [ 3.58663321e-01, -6.27704002e-01,  8.59054966e-01,\n",
       "         9.14069966e-01]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "features = iris.data[:100, :]\n",
    "target = iris.target[:100]\n",
    "\n",
    "scale = StandardScaler()\n",
    "scaled = scale.fit_transform(features)\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "model = lr.fit(scaled, target)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation = [[0.5, 0.4, 0.2, 0.7]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(new_observation)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18259902, 0.81740098]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = model.predict_proba(new_observation)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dispite having \"regression\" in its name, a logistic regression is actually a widely used binary lassifier (i.e. the target vector can only take two values). In a logistic regression, a linear model (e.g. $\\beta_0 + \\beta_i x$) is included in a logistic (also called sigmoid) function, $\\frac{1}{1+e^{-z }}$, such that:\n",
    "$$\n",
    "P(y_i = 1 | X) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x)}}\n",
    "$$\n",
    "where $P(y_i = 1 | X)$ is the probability of the ith obsevation's target, $y_i$ being class 1, X is the training data, $\\beta_0$ and $\\beta_1$ are the parameters to be learned, and e is Euler's number. The effect of the logistic function is to constrain the value of the function's output to between 0 and 1 so that i can be interpreted as a probability. If $P(y_i = 1 | X)$ is greater than 0.5, class 1 is predicted; otherwise class 0 is predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "scaled = scale.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='ovr', random_state=42)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OVR one-vs-rest Logistic Regression\n",
    "ovr = LogisticRegression(random_state = 42, multi_class = 'ovr')\n",
    "model = ovr.fit(scaled, target)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On their own, logistic regressions are only binary classifiers, meaning they cannot handle target vectors with more than two classes. However, two clever extensions to logistic regression do just that. \n",
    "\n",
    "`First, in one-vs-rest logistic regression (OVR)` a separate model is trained for each class predicted whether an observation is that class or not (thus making it a binary classification problem). It assumes that each observation problem (e.g. class 0 or not) is independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation = [[0.7, 0.5, 0.2, 0.7]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(new_observation)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04538717, 0.34272702, 0.61188581]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = model.predict_proba(new_observation)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial Logistic regression\n",
    "#logistic_regression_MNL = LogisticRegression(random_state=0, multi_class=\"multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Alternatively in multinomial logistic regression (MLR)` the logistic function  is replaced with a softmax function:\n",
    "$$\n",
    "P(y_I = k | X) = \\frac{e^{\\beta_k x_i}}{\\sum_{j=1}^{K}{e^{\\beta_j x_i}}}\n",
    "$$\n",
    "where $P(y_i = k | X)$ is the probability of the ith observation's target value, $y_i$, is class k, and K is the total number of classes. One practical advantage of the MLR is that its predicted probabilities using `predict_proba` method are more reliable\n",
    "\n",
    "We can switch to an MNL by setting `multi_class='multinomial'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(multi_class='multinomial', random_state=42)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlr = LogisticRegression(random_state = 42, multi_class = 'multinomial')\n",
    "model = mlr.fit(scaled, target)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation = [[0.7, 0.5, 0.2, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(new_observation)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01923294, 0.76695828, 0.21380879]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = model.predict_proba(new_observation)\n",
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Variance Through Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "scaled = scale.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_cv = LogisticRegressionCV(penalty = 'l2',\n",
    "                            Cs = 10,\n",
    "                            random_state = 42,\n",
    "                            n_jobs = -1)\n",
    "model =lr_cv.fit(scaled, target)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation = [[0.7, 0.5, 0.2, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.86627251e-04, 9.91050788e-01, 8.56258465e-03]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be seen that the probability has increased considerably after applying l2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a method of penalizing complex models to reduce their variance. Specifically, a penalty term is added to the loss function we are trying to minimize typically the L1 and L2 penalties\n",
    "\n",
    "In the L1 penalty:\n",
    "$$\n",
    "\\alpha \\sum_{j=1}^{p}{|\\hat\\beta_j|}\n",
    "$$\n",
    "where $\\hat\\beta_j$ is the parameters of the jth of p features being learned and $\\alpha$ is a hyperparameter denoting the regularization strength.\n",
    "\n",
    "With the L2 penalty:\n",
    "$$\n",
    "\\alpha \\sum_{j=1}^{p}{\\hat\\beta_j^2}\n",
    "$$\n",
    "higher values of $\\alpha$ increase the penalty for larger parameter values(i.e. more complex models). scikit-learn follows the common method of using C instead of $\\alpha$ where C is the inverse of the regularization strength: $C = \\frac{1}{\\alpha}$. To reduce variance while using logistic regression, we can treat C as a hyperparameter to be tuned to find the value of C that creates the best model. In scikit-learn we can use the `LogisticRegressionCV` class to efficiently tune C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Classifier on Very Large Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=42, solver='sag')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state = 42, solver=\"sag\") # stochastic average gradient (SAG) solver\n",
    "model = logistic_regression.fit(features_standardized, target)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn's `LogisticRegression` offers a number of techniques for training a logistic regression, called solvers. Most of the time scikit-learn will select the best solver automatically for us or warn us we cannot do something with that solver.\n",
    "\n",
    "`Stochastic averge gradient descent` allows us to train a model much faster than other solvers when our data is very large. However, it is also very sensitive to feature scaling, so standardizing our features is particularly important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Imbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data[40:, :]\n",
    "target = iris.target[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.where((target == 0), 0 ,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "scaled = scale.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', random_state=42)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression = LogisticRegression(random_state = 42, class_weight = \"balanced\")\n",
    "model = logistic_regression.fit(scaled, target)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` comes with a built in method of handling imbalanced classes.\n",
    "`class_weight=\"balanced\"` will automatically weigh classes inversely proportional to their frequency:\n",
    "$$\n",
    "w_j = \\frac{n}{kn_j}\n",
    "$$\n",
    "where $w_j$ is the weight to class j, n is the number of observations, $n_j$ is the number of observations in class j, and k is the total number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`An observation is predicted to be the class of that of the largest proportion of the k-nearest observations.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding an Observation's Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "features = iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "scaled = scale.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(n_neighbors=2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbors = NearestNeighbors(n_neighbors = 2).fit(scaled)\n",
    "nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_observation = [[1, 1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.49140089, 0.74294782]]), array([[124, 110]], dtype=int64))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances, indices = nearest_neighbors.kneighbors(new_observation)\n",
    "(distances, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49140089, 0.74294782]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124, 110]], dtype=int64)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.03800476, 0.55861082, 1.10378283, 1.18556721],\n",
       "        [0.79566902, 0.32841405, 0.76275827, 1.05393502]]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we measure distance?\n",
    "\n",
    "* Euclidian\n",
    "$$\n",
    "d_{euclidean} = \\sqrt{\\sum_{i=1}^{n}{(x_i - y_i)^2}}\n",
    "$$\n",
    "\n",
    "* Manhattan\n",
    "$$\n",
    "d_{manhattan} = \\sum_{i=1}^{n}{|x_i - y_i|}\n",
    "$$\n",
    "\n",
    "* Minkowski (default)\n",
    "$$\n",
    "d_{minkowski} = (\\sum_{i=1}^{n}{|x_i - y_i|^p})^{\\frac{1}{p}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a K-Nearest Neighbor Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
